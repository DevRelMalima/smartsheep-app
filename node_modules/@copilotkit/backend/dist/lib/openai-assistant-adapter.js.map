{"version":3,"sources":["../../src/lib/openai-assistant-adapter.ts","../../src/utils/openai.ts"],"sourcesContent":["import OpenAI from \"openai\";\nimport { CopilotKitServiceAdapter, CopilotKitResponse } from \"../types/service-adapter\";\nimport { writeChatCompletionChunk, writeChatCompletionEnd } from \"../utils/openai\";\nimport { ChatCompletionChunk, Message } from \"@copilotkit/shared\";\n\nconst RUN_STATUS_POLL_INTERVAL = 100;\n\nexport interface OpenAIAssistantAdapterParams {\n  assistantId: string;\n  openai?: OpenAI;\n  codeInterpreterEnabled?: boolean;\n  retrievalEnabled?: boolean;\n}\n\nexport class OpenAIAssistantAdapter implements CopilotKitServiceAdapter {\n  private openai: OpenAI;\n  private codeInterpreterEnabled: boolean;\n  private assistantId: string;\n  private retrievalEnabled: boolean;\n\n  constructor(params: OpenAIAssistantAdapterParams) {\n    this.openai = params.openai || new OpenAI({});\n    this.codeInterpreterEnabled = params.codeInterpreterEnabled === false || true;\n    this.retrievalEnabled = params.retrievalEnabled === false || true;\n    this.assistantId = params.assistantId;\n  }\n\n  async waitForRun(run: OpenAI.Beta.Threads.Runs.Run): Promise<OpenAI.Beta.Threads.Runs.Run> {\n    while (true) {\n      const status = await this.openai.beta.threads.runs.retrieve(run.thread_id, run.id);\n      if (status.status === \"completed\" || status.status === \"requires_action\") {\n        return status;\n      } else if (status.status !== \"in_progress\" && status.status !== \"queued\") {\n        console.error(`Thread run failed with status: ${status.status}`);\n        throw new Error(`Thread run failed with status: ${status.status}`);\n      }\n      await new Promise((resolve) => setTimeout(resolve, RUN_STATUS_POLL_INTERVAL));\n    }\n  }\n\n  async submitToolOutputs(threadId: string, runId: string, forwardMessages: Message[]) {\n    let run = await this.openai.beta.threads.runs.retrieve(threadId, runId);\n\n    if (!run.required_action) {\n      throw new Error(\"No tool outputs required\");\n    }\n\n    const functionResults: Message[] = [];\n    // get all function results at the tail of the messages\n    let i = forwardMessages.length - 1;\n    for (; i >= 0; i--) {\n      if (forwardMessages[i].role === \"function\") {\n        functionResults.unshift(forwardMessages[i]);\n      } else {\n        break;\n      }\n    }\n\n    const toolCallsIds = run.required_action.submit_tool_outputs.tool_calls.map(\n      (toolCall) => toolCall.id,\n    );\n\n    if (toolCallsIds.length != functionResults.length) {\n      throw new Error(\"Number of function results does not match the number of tool calls\");\n    }\n\n    const toolOutputs: any[] = [];\n\n    // match tool ids with function results\n    for (let i = 0; i < functionResults.length; i++) {\n      const toolCallId = toolCallsIds[i];\n      const functionResult = functionResults[i];\n      toolOutputs.push({\n        tool_call_id: toolCallId,\n        output: functionResult.content || \"\",\n      });\n    }\n\n    run = await this.openai.beta.threads.runs.submitToolOutputs(threadId, runId, {\n      tool_outputs: toolOutputs,\n    });\n\n    return await this.waitForRun(run);\n  }\n\n  async submitUserMessage(threadId: string, forwardedProps: any) {\n    const forwardMessages = forwardedProps.messages || [];\n\n    const message = forwardMessages[forwardMessages.length - 1];\n    await this.openai.beta.threads.messages.create(threadId, {\n      role: message.role as \"user\",\n      content: message.content,\n    });\n\n    const tools = [\n      ...(forwardedProps.tools || []),\n      ...(this.codeInterpreterEnabled ? [{ type: \"code_interpreter\" }] : []),\n      ...(this.retrievalEnabled ? [{ type: \"retrieval\" }] : []),\n    ];\n\n    // build instructions by joining all system messages\n    const instructions = forwardMessages\n      .filter((message: Message) => message.role === \"system\")\n      .map((message: Message) => message.content)\n      .join(\"\\n\\n\");\n\n    // run the thread\n    let run = await this.openai.beta.threads.runs.create(threadId, {\n      assistant_id: this.assistantId,\n      instructions,\n      tools: tools,\n    });\n\n    return await this.waitForRun(run);\n  }\n\n  async getResponse(forwardedProps: any): Promise<CopilotKitResponse> {\n    // copy forwardedProps to avoid modifying the original object\n    forwardedProps = { ...forwardedProps };\n\n    const forwardMessages = forwardedProps.messages || [];\n\n    // Remove tools if there are none to avoid OpenAI API errors\n    // when sending an empty array of tools\n    if (forwardedProps.tools && forwardedProps.tools.length === 0) {\n      delete forwardedProps.tools;\n    }\n\n    // get the thread from forwardedProps or create a new one\n    const threadId: string =\n      forwardedProps.threadId || (await this.openai.beta.threads.create()).id;\n\n    let run: OpenAI.Beta.Threads.Runs.Run | null = null;\n\n    // submit function outputs\n    if (\n      forwardMessages.length > 0 &&\n      forwardMessages[forwardMessages.length - 1].role === \"function\"\n    ) {\n      run = await this.submitToolOutputs(threadId, forwardedProps.runId, forwardMessages);\n    }\n    // submit user message\n    else if (\n      forwardMessages.length > 0 &&\n      forwardMessages[forwardMessages.length - 1].role === \"user\"\n    ) {\n      run = await this.submitUserMessage(threadId, forwardedProps);\n    }\n    // unsupported message\n    else {\n      console.error(\"No actionable message found in the messages\");\n      throw new Error(\"No actionable message found in the messages\");\n    }\n\n    if (run.status === \"requires_action\") {\n      // return the tool calls\n      return {\n        stream: new AssistantSingleChunkReadableStream(\n          \"\",\n          run.required_action!.submit_tool_outputs.tool_calls,\n        ),\n        headers: { threadId, runId: run.id },\n      };\n    } else {\n      // return the last message\n      const newMessages = await this.openai.beta.threads.messages.list(threadId, {\n        limit: 1,\n        order: \"desc\",\n      });\n\n      const content = newMessages.data[0].content[0];\n      const contentString = content.type === \"text\" ? content.text.value : \"\";\n\n      return {\n        stream: new AssistantSingleChunkReadableStream(contentString),\n        headers: { threadId },\n      };\n    }\n  }\n}\n\nclass AssistantSingleChunkReadableStream extends ReadableStream<any> {\n  constructor(\n    content: string,\n    toolCalls?: OpenAI.Beta.Threads.Runs.RequiredActionFunctionToolCall[],\n  ) {\n    super({\n      start(controller) {\n        let tool_calls: any = undefined;\n        if (toolCalls) {\n          tool_calls = toolCalls.map((toolCall, index) => {\n            return {\n              index,\n              id: toolCall.id,\n              function: {\n                name: toolCall.function.name,\n                arguments: toolCall.function.arguments,\n              },\n            };\n          });\n        }\n        const chunk: ChatCompletionChunk = {\n          choices: [\n            {\n              delta: {\n                content: content,\n                role: \"assistant\",\n                tool_calls,\n              },\n            },\n          ],\n        };\n        writeChatCompletionChunk(controller, chunk);\n        writeChatCompletionEnd(controller);\n\n        controller.close();\n      },\n      cancel() {},\n    });\n  }\n}\n","import { Message, ToolDefinition, ChatCompletionChunk, encodeResult } from \"@copilotkit/shared\";\n\nexport function writeChatCompletionChunk(\n  controller: ReadableStreamDefaultController<any>,\n  chunk: ChatCompletionChunk,\n) {\n  const payload = new TextEncoder().encode(\"data: \" + JSON.stringify(chunk) + \"\\n\\n\");\n  controller!.enqueue(payload);\n}\n\nexport function writeChatCompletionContent(\n  controller: ReadableStreamDefaultController<any>,\n  content: string = \"\",\n  toolCalls?: any,\n) {\n  const chunk: ChatCompletionChunk = {\n    choices: [\n      {\n        delta: {\n          role: \"assistant\",\n          content: content,\n          ...(toolCalls ? { tool_calls: toolCalls } : {}),\n        },\n      },\n    ],\n  };\n\n  writeChatCompletionChunk(controller, chunk);\n}\n\nexport function writeChatCompletionResult(\n  controller: ReadableStreamDefaultController<any>,\n  functionName: string,\n  result: any,\n) {\n  let resultString = encodeResult(result);\n\n  const chunk: ChatCompletionChunk = {\n    choices: [\n      {\n        delta: {\n          role: \"function\",\n          content: resultString,\n          name: functionName,\n        },\n      },\n    ],\n  };\n\n  writeChatCompletionChunk(controller, chunk);\n}\n\nexport function writeChatCompletionEnd(controller: ReadableStreamDefaultController<any>) {\n  const payload = new TextEncoder().encode(\"data: [DONE]\\n\\n\");\n  controller.enqueue(payload);\n}\n\nexport function limitOpenAIMessagesToTokenCount(\n  messages: Message[],\n  tools: ToolDefinition[],\n  maxTokens: number,\n): Message[] {\n  const result: Message[] = [];\n  const toolsNumTokens = countToolsTokens(tools);\n  if (toolsNumTokens > maxTokens) {\n    throw new Error(`Too many tokens in function definitions: ${toolsNumTokens} > ${maxTokens}`);\n  }\n  maxTokens -= toolsNumTokens;\n\n  for (const message of messages) {\n    if (message.role === \"system\") {\n      const numTokens = countMessageTokens(message);\n      maxTokens -= numTokens;\n\n      if (maxTokens < 0) {\n        throw new Error(\"Not enough tokens for system message.\");\n      }\n    }\n  }\n\n  let cutoff: boolean = false;\n\n  const reversedMessages = [...messages].reverse();\n  for (const message of reversedMessages) {\n    if (message.role === \"system\") {\n      result.unshift(message);\n      continue;\n    } else if (cutoff) {\n      continue;\n    }\n    let numTokens = countMessageTokens(message);\n    if (maxTokens < numTokens) {\n      cutoff = true;\n      continue;\n    }\n    result.unshift(message);\n    maxTokens -= numTokens;\n  }\n\n  return result;\n}\n\nexport function maxTokensForOpenAIModel(model: string): number {\n  return maxTokensByModel[model] || DEFAULT_MAX_TOKENS;\n}\n\nconst DEFAULT_MAX_TOKENS = 8192;\n\nconst maxTokensByModel: { [key: string]: number } = {\n  // GPT-4\n  \"gpt-4-0125-preview\": 128000,\n  \"gpt-4-turbo-preview\": 128000,\n  \"gpt-4-1106-preview\": 128000,\n  \"gpt-4-vision-preview\": 128000,\n  \"gpt-4-1106-vision-preview\": 128000,\n  \"gpt-4-32k\": 32768,\n  \"gpt-4-32k-0613\": 32768,\n  \"gpt-4-32k-0314\": 32768,\n  \"gpt-4\": 8192,\n  \"gpt-4-0613\": 8192,\n  \"gpt-4-0314\": 8192,\n\n  // GPT-3.5\n  \"gpt-3.5-turbo-0125\": 16385,\n  \"gpt-3.5-turbo\": 16385,\n  \"gpt-3.5-turbo-1106\": 16385,\n  \"gpt-3.5-turbo-instruct\": 4096,\n  \"gpt-3.5-turbo-16k\": 16385,\n  \"gpt-3.5-turbo-0613\": 4096,\n  \"gpt-3.5-turbo-16k-0613\": 16385,\n  \"gpt-3.5-turbo-0301\": 4097,\n};\n\nfunction countToolsTokens(functions: ToolDefinition[]): number {\n  if (functions.length === 0) {\n    return 0;\n  }\n  const json = JSON.stringify(functions);\n  return countTokens(json);\n}\n\nfunction countMessageTokens(message: Message): number {\n  if (message.content) {\n    return countTokens(message.content);\n  } else if (message.function_call) {\n    return countTokens(JSON.stringify(message.function_call));\n  }\n  return 0;\n}\n\nfunction countTokens(text: string): number {\n  return text.length / 3;\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAAmB;;;ACAnB,oBAA2E;AAEpE,SAAS,yBACd,YACA,OACA;AACA,QAAM,UAAU,IAAI,YAAY,EAAE,OAAO,WAAW,KAAK,UAAU,KAAK,IAAI,MAAM;AAClF,aAAY,QAAQ,OAAO;AAC7B;AA4CO,SAAS,uBAAuB,YAAkD;AACvF,QAAM,UAAU,IAAI,YAAY,EAAE,OAAO,kBAAkB;AAC3D,aAAW,QAAQ,OAAO;AAC5B;;;ADlDA,IAAM,2BAA2B;AAS1B,IAAM,yBAAN,MAAiE;AAAA,EAMtE,YAAY,QAAsC;AAChD,SAAK,SAAS,OAAO,UAAU,IAAI,cAAAA,QAAO,CAAC,CAAC;AAC5C,SAAK,yBAAyB,OAAO,2BAA2B,SAAS;AACzE,SAAK,mBAAmB,OAAO,qBAAqB,SAAS;AAC7D,SAAK,cAAc,OAAO;AAAA,EAC5B;AAAA,EAEA,MAAM,WAAW,KAA0E;AACzF,WAAO,MAAM;AACX,YAAM,SAAS,MAAM,KAAK,OAAO,KAAK,QAAQ,KAAK,SAAS,IAAI,WAAW,IAAI,EAAE;AACjF,UAAI,OAAO,WAAW,eAAe,OAAO,WAAW,mBAAmB;AACxE,eAAO;AAAA,MACT,WAAW,OAAO,WAAW,iBAAiB,OAAO,WAAW,UAAU;AACxE,gBAAQ,MAAM,kCAAkC,OAAO,QAAQ;AAC/D,cAAM,IAAI,MAAM,kCAAkC,OAAO,QAAQ;AAAA,MACnE;AACA,YAAM,IAAI,QAAQ,CAAC,YAAY,WAAW,SAAS,wBAAwB,CAAC;AAAA,IAC9E;AAAA,EACF;AAAA,EAEA,MAAM,kBAAkB,UAAkB,OAAe,iBAA4B;AACnF,QAAI,MAAM,MAAM,KAAK,OAAO,KAAK,QAAQ,KAAK,SAAS,UAAU,KAAK;AAEtE,QAAI,CAAC,IAAI,iBAAiB;AACxB,YAAM,IAAI,MAAM,0BAA0B;AAAA,IAC5C;AAEA,UAAM,kBAA6B,CAAC;AAEpC,QAAI,IAAI,gBAAgB,SAAS;AACjC,WAAO,KAAK,GAAG,KAAK;AAClB,UAAI,gBAAgB,CAAC,EAAE,SAAS,YAAY;AAC1C,wBAAgB,QAAQ,gBAAgB,CAAC,CAAC;AAAA,MAC5C,OAAO;AACL;AAAA,MACF;AAAA,IACF;AAEA,UAAM,eAAe,IAAI,gBAAgB,oBAAoB,WAAW;AAAA,MACtE,CAAC,aAAa,SAAS;AAAA,IACzB;AAEA,QAAI,aAAa,UAAU,gBAAgB,QAAQ;AACjD,YAAM,IAAI,MAAM,oEAAoE;AAAA,IACtF;AAEA,UAAM,cAAqB,CAAC;AAG5B,aAASC,KAAI,GAAGA,KAAI,gBAAgB,QAAQA,MAAK;AAC/C,YAAM,aAAa,aAAaA,EAAC;AACjC,YAAM,iBAAiB,gBAAgBA,EAAC;AACxC,kBAAY,KAAK;AAAA,QACf,cAAc;AAAA,QACd,QAAQ,eAAe,WAAW;AAAA,MACpC,CAAC;AAAA,IACH;AAEA,UAAM,MAAM,KAAK,OAAO,KAAK,QAAQ,KAAK,kBAAkB,UAAU,OAAO;AAAA,MAC3E,cAAc;AAAA,IAChB,CAAC;AAED,WAAO,MAAM,KAAK,WAAW,GAAG;AAAA,EAClC;AAAA,EAEA,MAAM,kBAAkB,UAAkB,gBAAqB;AAC7D,UAAM,kBAAkB,eAAe,YAAY,CAAC;AAEpD,UAAM,UAAU,gBAAgB,gBAAgB,SAAS,CAAC;AAC1D,UAAM,KAAK,OAAO,KAAK,QAAQ,SAAS,OAAO,UAAU;AAAA,MACvD,MAAM,QAAQ;AAAA,MACd,SAAS,QAAQ;AAAA,IACnB,CAAC;AAED,UAAM,QAAQ;AAAA,MACZ,GAAI,eAAe,SAAS,CAAC;AAAA,MAC7B,GAAI,KAAK,yBAAyB,CAAC,EAAE,MAAM,mBAAmB,CAAC,IAAI,CAAC;AAAA,MACpE,GAAI,KAAK,mBAAmB,CAAC,EAAE,MAAM,YAAY,CAAC,IAAI,CAAC;AAAA,IACzD;AAGA,UAAM,eAAe,gBAClB,OAAO,CAACC,aAAqBA,SAAQ,SAAS,QAAQ,EACtD,IAAI,CAACA,aAAqBA,SAAQ,OAAO,EACzC,KAAK,MAAM;AAGd,QAAI,MAAM,MAAM,KAAK,OAAO,KAAK,QAAQ,KAAK,OAAO,UAAU;AAAA,MAC7D,cAAc,KAAK;AAAA,MACnB;AAAA,MACA;AAAA,IACF,CAAC;AAED,WAAO,MAAM,KAAK,WAAW,GAAG;AAAA,EAClC;AAAA,EAEA,MAAM,YAAY,gBAAkD;AAElE,qBAAiB,EAAE,GAAG,eAAe;AAErC,UAAM,kBAAkB,eAAe,YAAY,CAAC;AAIpD,QAAI,eAAe,SAAS,eAAe,MAAM,WAAW,GAAG;AAC7D,aAAO,eAAe;AAAA,IACxB;AAGA,UAAM,WACJ,eAAe,aAAa,MAAM,KAAK,OAAO,KAAK,QAAQ,OAAO,GAAG;AAEvE,QAAI,MAA2C;AAG/C,QACE,gBAAgB,SAAS,KACzB,gBAAgB,gBAAgB,SAAS,CAAC,EAAE,SAAS,YACrD;AACA,YAAM,MAAM,KAAK,kBAAkB,UAAU,eAAe,OAAO,eAAe;AAAA,IACpF,WAGE,gBAAgB,SAAS,KACzB,gBAAgB,gBAAgB,SAAS,CAAC,EAAE,SAAS,QACrD;AACA,YAAM,MAAM,KAAK,kBAAkB,UAAU,cAAc;AAAA,IAC7D,OAEK;AACH,cAAQ,MAAM,6CAA6C;AAC3D,YAAM,IAAI,MAAM,6CAA6C;AAAA,IAC/D;AAEA,QAAI,IAAI,WAAW,mBAAmB;AAEpC,aAAO;AAAA,QACL,QAAQ,IAAI;AAAA,UACV;AAAA,UACA,IAAI,gBAAiB,oBAAoB;AAAA,QAC3C;AAAA,QACA,SAAS,EAAE,UAAU,OAAO,IAAI,GAAG;AAAA,MACrC;AAAA,IACF,OAAO;AAEL,YAAM,cAAc,MAAM,KAAK,OAAO,KAAK,QAAQ,SAAS,KAAK,UAAU;AAAA,QACzE,OAAO;AAAA,QACP,OAAO;AAAA,MACT,CAAC;AAED,YAAM,UAAU,YAAY,KAAK,CAAC,EAAE,QAAQ,CAAC;AAC7C,YAAM,gBAAgB,QAAQ,SAAS,SAAS,QAAQ,KAAK,QAAQ;AAErE,aAAO;AAAA,QACL,QAAQ,IAAI,mCAAmC,aAAa;AAAA,QAC5D,SAAS,EAAE,SAAS;AAAA,MACtB;AAAA,IACF;AAAA,EACF;AACF;AAEA,IAAM,qCAAN,cAAiD,eAAoB;AAAA,EACnE,YACE,SACA,WACA;AACA,UAAM;AAAA,MACJ,MAAM,YAAY;AAChB,YAAI,aAAkB;AACtB,YAAI,WAAW;AACb,uBAAa,UAAU,IAAI,CAAC,UAAU,UAAU;AAC9C,mBAAO;AAAA,cACL;AAAA,cACA,IAAI,SAAS;AAAA,cACb,UAAU;AAAA,gBACR,MAAM,SAAS,SAAS;AAAA,gBACxB,WAAW,SAAS,SAAS;AAAA,cAC/B;AAAA,YACF;AAAA,UACF,CAAC;AAAA,QACH;AACA,cAAM,QAA6B;AAAA,UACjC,SAAS;AAAA,YACP;AAAA,cACE,OAAO;AAAA,gBACL;AAAA,gBACA,MAAM;AAAA,gBACN;AAAA,cACF;AAAA,YACF;AAAA,UACF;AAAA,QACF;AACA,iCAAyB,YAAY,KAAK;AAC1C,+BAAuB,UAAU;AAEjC,mBAAW,MAAM;AAAA,MACnB;AAAA,MACA,SAAS;AAAA,MAAC;AAAA,IACZ,CAAC;AAAA,EACH;AACF;","names":["OpenAI","i","message"]}