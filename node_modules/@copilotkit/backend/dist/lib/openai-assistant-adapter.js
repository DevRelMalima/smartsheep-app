"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/lib/openai-assistant-adapter.ts
var openai_assistant_adapter_exports = {};
__export(openai_assistant_adapter_exports, {
  OpenAIAssistantAdapter: () => OpenAIAssistantAdapter
});
module.exports = __toCommonJS(openai_assistant_adapter_exports);
var import_openai = __toESM(require("openai"));

// src/utils/openai.ts
var import_shared = require("@copilotkit/shared");
function writeChatCompletionChunk(controller, chunk) {
  const payload = new TextEncoder().encode("data: " + JSON.stringify(chunk) + "\n\n");
  controller.enqueue(payload);
}
function writeChatCompletionEnd(controller) {
  const payload = new TextEncoder().encode("data: [DONE]\n\n");
  controller.enqueue(payload);
}

// src/lib/openai-assistant-adapter.ts
var RUN_STATUS_POLL_INTERVAL = 100;
var OpenAIAssistantAdapter = class {
  constructor(params) {
    this.openai = params.openai || new import_openai.default({});
    this.codeInterpreterEnabled = params.codeInterpreterEnabled === false || true;
    this.retrievalEnabled = params.retrievalEnabled === false || true;
    this.assistantId = params.assistantId;
  }
  async waitForRun(run) {
    while (true) {
      const status = await this.openai.beta.threads.runs.retrieve(run.thread_id, run.id);
      if (status.status === "completed" || status.status === "requires_action") {
        return status;
      } else if (status.status !== "in_progress" && status.status !== "queued") {
        console.error(`Thread run failed with status: ${status.status}`);
        throw new Error(`Thread run failed with status: ${status.status}`);
      }
      await new Promise((resolve) => setTimeout(resolve, RUN_STATUS_POLL_INTERVAL));
    }
  }
  async submitToolOutputs(threadId, runId, forwardMessages) {
    let run = await this.openai.beta.threads.runs.retrieve(threadId, runId);
    if (!run.required_action) {
      throw new Error("No tool outputs required");
    }
    const functionResults = [];
    let i = forwardMessages.length - 1;
    for (; i >= 0; i--) {
      if (forwardMessages[i].role === "function") {
        functionResults.unshift(forwardMessages[i]);
      } else {
        break;
      }
    }
    const toolCallsIds = run.required_action.submit_tool_outputs.tool_calls.map(
      (toolCall) => toolCall.id
    );
    if (toolCallsIds.length != functionResults.length) {
      throw new Error("Number of function results does not match the number of tool calls");
    }
    const toolOutputs = [];
    for (let i2 = 0; i2 < functionResults.length; i2++) {
      const toolCallId = toolCallsIds[i2];
      const functionResult = functionResults[i2];
      toolOutputs.push({
        tool_call_id: toolCallId,
        output: functionResult.content || ""
      });
    }
    run = await this.openai.beta.threads.runs.submitToolOutputs(threadId, runId, {
      tool_outputs: toolOutputs
    });
    return await this.waitForRun(run);
  }
  async submitUserMessage(threadId, forwardedProps) {
    const forwardMessages = forwardedProps.messages || [];
    const message = forwardMessages[forwardMessages.length - 1];
    await this.openai.beta.threads.messages.create(threadId, {
      role: message.role,
      content: message.content
    });
    const tools = [
      ...forwardedProps.tools || [],
      ...this.codeInterpreterEnabled ? [{ type: "code_interpreter" }] : [],
      ...this.retrievalEnabled ? [{ type: "retrieval" }] : []
    ];
    const instructions = forwardMessages.filter((message2) => message2.role === "system").map((message2) => message2.content).join("\n\n");
    let run = await this.openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId,
      instructions,
      tools
    });
    return await this.waitForRun(run);
  }
  async getResponse(forwardedProps) {
    forwardedProps = { ...forwardedProps };
    const forwardMessages = forwardedProps.messages || [];
    if (forwardedProps.tools && forwardedProps.tools.length === 0) {
      delete forwardedProps.tools;
    }
    const threadId = forwardedProps.threadId || (await this.openai.beta.threads.create()).id;
    let run = null;
    if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "function") {
      run = await this.submitToolOutputs(threadId, forwardedProps.runId, forwardMessages);
    } else if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "user") {
      run = await this.submitUserMessage(threadId, forwardedProps);
    } else {
      console.error("No actionable message found in the messages");
      throw new Error("No actionable message found in the messages");
    }
    if (run.status === "requires_action") {
      return {
        stream: new AssistantSingleChunkReadableStream(
          "",
          run.required_action.submit_tool_outputs.tool_calls
        ),
        headers: { threadId, runId: run.id }
      };
    } else {
      const newMessages = await this.openai.beta.threads.messages.list(threadId, {
        limit: 1,
        order: "desc"
      });
      const content = newMessages.data[0].content[0];
      const contentString = content.type === "text" ? content.text.value : "";
      return {
        stream: new AssistantSingleChunkReadableStream(contentString),
        headers: { threadId }
      };
    }
  }
};
var AssistantSingleChunkReadableStream = class extends ReadableStream {
  constructor(content, toolCalls) {
    super({
      start(controller) {
        let tool_calls = void 0;
        if (toolCalls) {
          tool_calls = toolCalls.map((toolCall, index) => {
            return {
              index,
              id: toolCall.id,
              function: {
                name: toolCall.function.name,
                arguments: toolCall.function.arguments
              }
            };
          });
        }
        const chunk = {
          choices: [
            {
              delta: {
                content,
                role: "assistant",
                tool_calls
              }
            }
          ]
        };
        writeChatCompletionChunk(controller, chunk);
        writeChatCompletionEnd(controller);
        controller.close();
      },
      cancel() {
      }
    });
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  OpenAIAssistantAdapter
});
//# sourceMappingURL=openai-assistant-adapter.js.map