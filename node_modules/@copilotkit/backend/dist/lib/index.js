"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/lib/index.ts
var lib_exports = {};
__export(lib_exports, {
  CopilotBackend: () => CopilotBackend,
  LangChainAdapter: () => LangChainAdapter,
  OpenAIAdapter: () => OpenAIAdapter,
  OpenAIAssistantAdapter: () => OpenAIAssistantAdapter
});
module.exports = __toCommonJS(lib_exports);

// src/lib/copilotkit-backend-implementation.ts
var import_shared3 = require("@copilotkit/shared");

// src/utils/openai.ts
var import_shared = require("@copilotkit/shared");
function writeChatCompletionChunk(controller, chunk) {
  const payload = new TextEncoder().encode("data: " + JSON.stringify(chunk) + "\n\n");
  controller.enqueue(payload);
}
function writeChatCompletionContent(controller, content = "", toolCalls) {
  const chunk = {
    choices: [
      {
        delta: {
          role: "assistant",
          content,
          ...toolCalls ? { tool_calls: toolCalls } : {}
        }
      }
    ]
  };
  writeChatCompletionChunk(controller, chunk);
}
function writeChatCompletionResult(controller, functionName, result) {
  let resultString = (0, import_shared.encodeResult)(result);
  const chunk = {
    choices: [
      {
        delta: {
          role: "function",
          content: resultString,
          name: functionName
        }
      }
    ]
  };
  writeChatCompletionChunk(controller, chunk);
}
function writeChatCompletionEnd(controller) {
  const payload = new TextEncoder().encode("data: [DONE]\n\n");
  controller.enqueue(payload);
}
function limitOpenAIMessagesToTokenCount(messages, tools, maxTokens) {
  const result = [];
  const toolsNumTokens = countToolsTokens(tools);
  if (toolsNumTokens > maxTokens) {
    throw new Error(`Too many tokens in function definitions: ${toolsNumTokens} > ${maxTokens}`);
  }
  maxTokens -= toolsNumTokens;
  for (const message of messages) {
    if (message.role === "system") {
      const numTokens = countMessageTokens(message);
      maxTokens -= numTokens;
      if (maxTokens < 0) {
        throw new Error("Not enough tokens for system message.");
      }
    }
  }
  let cutoff = false;
  const reversedMessages = [...messages].reverse();
  for (const message of reversedMessages) {
    if (message.role === "system") {
      result.unshift(message);
      continue;
    } else if (cutoff) {
      continue;
    }
    let numTokens = countMessageTokens(message);
    if (maxTokens < numTokens) {
      cutoff = true;
      continue;
    }
    result.unshift(message);
    maxTokens -= numTokens;
  }
  return result;
}
function maxTokensForOpenAIModel(model) {
  return maxTokensByModel[model] || DEFAULT_MAX_TOKENS;
}
var DEFAULT_MAX_TOKENS = 8192;
var maxTokensByModel = {
  // GPT-4
  "gpt-4-0125-preview": 128e3,
  "gpt-4-turbo-preview": 128e3,
  "gpt-4-1106-preview": 128e3,
  "gpt-4-vision-preview": 128e3,
  "gpt-4-1106-vision-preview": 128e3,
  "gpt-4-32k": 32768,
  "gpt-4-32k-0613": 32768,
  "gpt-4-32k-0314": 32768,
  "gpt-4": 8192,
  "gpt-4-0613": 8192,
  "gpt-4-0314": 8192,
  // GPT-3.5
  "gpt-3.5-turbo-0125": 16385,
  "gpt-3.5-turbo": 16385,
  "gpt-3.5-turbo-1106": 16385,
  "gpt-3.5-turbo-instruct": 4096,
  "gpt-3.5-turbo-16k": 16385,
  "gpt-3.5-turbo-0613": 4096,
  "gpt-3.5-turbo-16k-0613": 16385,
  "gpt-3.5-turbo-0301": 4097
};
function countToolsTokens(functions) {
  if (functions.length === 0) {
    return 0;
  }
  const json = JSON.stringify(functions);
  return countTokens(json);
}
function countMessageTokens(message) {
  if (message.content) {
    return countTokens(message.content);
  } else if (message.function_call) {
    return countTokens(JSON.stringify(message.function_call));
  }
  return 0;
}
function countTokens(text) {
  return text.length / 3;
}

// src/utils/stream.ts
var import_shared2 = require("@copilotkit/shared");
async function executeFunctionCall(controller, action, functionCallArguments) {
  var _a, _b, _c, _d, _e, _f;
  let args = [];
  if (functionCallArguments) {
    args = JSON.parse(functionCallArguments);
  }
  const result = await action.handler(args);
  if (typeof result === "string") {
    writeChatCompletionResult(controller, action.name, result);
  } else if ("content" in result && typeof result.content === "string") {
    writeChatCompletionContent(controller, result.content, (_a = result.additional_kwargs) == null ? void 0 : _a.tool_calls);
  } else if ("lc_kwargs" in result) {
    writeChatCompletionContent(controller, (_b = result.lc_kwargs) == null ? void 0 : _b.content, (_c = result.lc_kwargs) == null ? void 0 : _c.tool_calls);
  } else if ("getReader" in result) {
    let reader = result.getReader();
    while (true) {
      try {
        const { done, value } = await reader.read();
        if (done) {
          break;
        }
        writeChatCompletionContent(
          controller,
          (_d = value == null ? void 0 : value.lc_kwargs) == null ? void 0 : _d.content,
          (_f = (_e = value.lc_kwargs) == null ? void 0 : _e.additional_kwargs) == null ? void 0 : _f.tool_calls
        );
      } catch (error) {
        console.error("Error reading from stream", error);
        break;
      }
    }
  } else {
    writeChatCompletionResult(controller, action.name, result);
  }
}
function copilotkitStreamInterceptor(stream, actions, debug = false) {
  const functionsByName = actions.reduce((acc, fn) => {
    acc[fn.name] = fn;
    return acc;
  }, {});
  const decodedStream = (0, import_shared2.parseChatCompletion)(stream);
  const reader = decodedStream.getReader();
  async function cleanup(controller) {
    if (controller) {
      try {
        controller.close();
      } catch (_) {
      }
    }
    if (reader) {
      try {
        await reader.cancel();
      } catch (_) {
      }
    }
  }
  let executeThisFunctionCall = false;
  let functionCallName = "";
  let functionCallArguments = "";
  let currentFnIndex = 0;
  const flushFunctionCall = async (controller) => {
    const action = functionsByName[functionCallName];
    await executeFunctionCall(controller, action, functionCallArguments);
    executeThisFunctionCall = false;
    functionCallName = "";
    functionCallArguments = "";
  };
  return new ReadableStream({
    async pull(controller) {
      var _a, _b, _c, _d, _e, _f, _g, _h;
      while (true) {
        try {
          const { done, value } = await reader.read();
          if (done) {
            if (debug) {
              console.log("data: [DONE]\n\n");
            }
            if (executeThisFunctionCall) {
              await flushFunctionCall(controller);
            }
            writeChatCompletionEnd(controller);
            await cleanup(controller);
            return;
          } else if (debug) {
            console.log("data: " + JSON.stringify(value) + "\n\n");
          }
          let mode = value.choices[0].delta.tool_calls ? "function" : "message";
          const index = ((_b = (_a = value.choices[0].delta.tool_calls) == null ? void 0 : _a[0]) == null ? void 0 : _b.index) || 0;
          if (executeThisFunctionCall && (mode != "function" || index != currentFnIndex)) {
            await flushFunctionCall(controller);
          }
          currentFnIndex = index;
          if (mode === "message") {
            if (value.choices[0].delta.content) {
              writeChatCompletionChunk(controller, value);
            }
            continue;
          } else if (mode === "function") {
            if ((_e = (_d = (_c = value.choices[0].delta.tool_calls) == null ? void 0 : _c[0]) == null ? void 0 : _d.function) == null ? void 0 : _e.name) {
              functionCallName = value.choices[0].delta.tool_calls[0].function.name;
            }
            if ((_h = (_g = (_f = value.choices[0].delta.tool_calls) == null ? void 0 : _f[0]) == null ? void 0 : _g.function) == null ? void 0 : _h.arguments) {
              functionCallArguments += value.choices[0].delta.tool_calls[0].function.arguments;
            }
            if (!executeThisFunctionCall) {
              if (functionCallName in functionsByName) {
                executeThisFunctionCall = true;
              }
            }
            if (value.choices[0].delta.tool_calls) {
              value.choices[0].delta.tool_calls[0].function.scope = executeThisFunctionCall ? "server" : "client";
            }
            writeChatCompletionChunk(controller, value);
            continue;
          }
        } catch (error) {
          controller.error(error);
          return;
        }
      }
    },
    cancel() {
      reader.cancel();
    }
  });
}

// src/utils/langserve.ts
var import_remote = require("langchain/runnables/remote");
async function remoteChainToAction(chain) {
  chain = { ...chain };
  const runnable = new import_remote.RemoteRunnable({ url: chain.chainUrl });
  if (!chain.parameters) {
    chain = await inferLangServeParameters(chain);
  }
  chain.parameterType || (chain.parameterType = "multi");
  return {
    name: chain.name,
    description: chain.description,
    parameters: chain.parameters,
    handler: async (args) => {
      let input;
      if (chain.parameterType === "single") {
        input = args[Object.keys(args)[0]];
      } else {
        input = args;
      }
      return await runnable.invoke(input);
    }
  };
}
async function inferLangServeParameters(chain) {
  chain = { ...chain };
  const supportedTypes = ["string", "number", "boolean"];
  let schemaUrl = chain.chainUrl.replace(/\/+$/, "") + "/input_schema";
  let schema = await fetch(schemaUrl).then((res) => res.json()).catch(() => {
    throw new Error("Failed to fetch langserve schema at " + schemaUrl);
  });
  if (supportedTypes.includes(schema.type)) {
    chain.parameterType = "single";
    chain.parameters = [
      {
        name: "input",
        type: schema.type,
        description: "The input to the chain"
      }
    ];
  } else if (schema.type === "object") {
    chain.parameterType = "multi";
    chain.parameters = Object.keys(schema.properties).map((key) => {
      var _a;
      let property = schema.properties[key];
      if (!supportedTypes.includes(property.type)) {
        throw new Error("Unsupported schema type");
      }
      return {
        name: key,
        type: property.type,
        description: property.description || "",
        required: ((_a = schema.required) == null ? void 0 : _a.includes(key)) || false
      };
    });
  } else {
    throw new Error("Unsupported schema type");
  }
  return chain;
}

// src/lib/copilotkit-backend-implementation.ts
var CopilotBackendImplementation = class {
  constructor(params) {
    this.actions = [];
    this.langserve = [];
    this.debug = false;
    for (const action of (params == null ? void 0 : params.actions) || []) {
      if ("argumentAnnotations" in action) {
        this.actions.push((0, import_shared3.annotatedFunctionToAction)(action));
      } else {
        this.actions.push(action);
      }
    }
    for (const chain of (params == null ? void 0 : params.langserve) || []) {
      this.langserve.push(remoteChainToAction(chain));
    }
    this.debug = (params == null ? void 0 : params.debug) || false;
  }
  addAction(action) {
    this.removeAction(action.name);
    if ("argumentAnnotations" in action) {
      this.actions.push((0, import_shared3.annotatedFunctionToAction)(action));
    } else {
      this.actions.push(action);
    }
  }
  removeAction(actionName) {
    this.actions = this.actions.filter((f) => f.name !== actionName);
  }
  removeBackendOnlyProps(forwardedProps) {
    const backendOnlyPropsKeys = forwardedProps[import_shared3.EXCLUDE_FROM_FORWARD_PROPS_KEYS];
    if (Array.isArray(backendOnlyPropsKeys)) {
      backendOnlyPropsKeys.forEach((key) => {
        const success2 = Reflect.deleteProperty(forwardedProps, key);
        if (!success2) {
          console.error(`Failed to delete property ${key}`);
        }
      });
      const success = Reflect.deleteProperty(forwardedProps, import_shared3.EXCLUDE_FROM_FORWARD_PROPS_KEYS);
      if (!success) {
        console.error(`Failed to delete EXCLUDE_FROM_FORWARD_PROPS_KEYS`);
      }
    } else if (backendOnlyPropsKeys) {
      console.error("backendOnlyPropsKeys is not an array");
    }
  }
  async getResponse(forwardedProps, serviceAdapter) {
    this.removeBackendOnlyProps(forwardedProps);
    const langserveFunctions = [];
    for (const chainPromise of this.langserve) {
      try {
        const chain = await chainPromise;
        langserveFunctions.push(chain);
      } catch (error) {
        console.error("Error loading langserve chain:", error);
      }
    }
    let mergedTools = mergeServerSideTools(
      this.actions.map(import_shared3.actionToChatCompletionFunction),
      langserveFunctions.map(import_shared3.actionToChatCompletionFunction)
    );
    mergedTools = mergeServerSideTools(mergedTools, forwardedProps.tools);
    try {
      const result = await serviceAdapter.getResponse({
        ...forwardedProps,
        tools: mergedTools
      });
      const stream = copilotkitStreamInterceptor(
        result.stream,
        [...this.actions, ...langserveFunctions],
        this.debug
      );
      return { stream, headers: result.headers };
    } catch (error) {
      console.error("Error getting response:", error);
      throw error;
    }
  }
  async response(req, serviceAdapter) {
    try {
      const response = await this.getResponse(await req.json(), serviceAdapter);
      return new Response(response.stream, { headers: response.headers });
    } catch (error) {
      return new Response("", { status: 500, statusText: error.message });
    }
  }
  async streamHttpServerResponse(req, res, serviceAdapter, headers) {
    const bodyParser = new Promise((resolve, reject) => {
      if ("body" in req) {
        resolve(req.body);
        return;
      }
      let body = "";
      req.on("data", (chunk) => body += chunk.toString());
      req.on("end", () => {
        try {
          resolve(JSON.parse(body));
        } catch (error) {
          reject(error);
        }
      });
    });
    const forwardedProps = await bodyParser;
    const response = await this.getResponse(forwardedProps, serviceAdapter);
    const mergedHeaders = { ...headers, ...response.headers };
    res.writeHead(200, mergedHeaders);
    const stream = response.stream;
    const reader = stream.getReader();
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        res.end();
        break;
      } else {
        res.write(new TextDecoder().decode(value));
      }
    }
  }
};
function mergeServerSideTools(serverTools, clientTools) {
  let allTools = serverTools.slice();
  const serverToolsNames = serverTools.map((tool) => tool.function.name);
  if (clientTools) {
    allTools = allTools.concat(
      // filter out any client functions that are already defined on the server
      clientTools.filter((tool) => !serverToolsNames.includes(tool.function.name))
    );
  }
  return allTools;
}

// src/lib/copilotkit-backend.ts
var CopilotBackend = class extends CopilotBackendImplementation {
  constructor(params) {
    super(params);
  }
  addAction(action) {
    super.addAction(action);
  }
};

// src/lib/openai-adapter.ts
var import_openai2 = __toESM(require("openai"));
var DEFAULT_MODEL = "gpt-4-1106-preview";
var OpenAIAdapter = class {
  constructor(params) {
    this.model = DEFAULT_MODEL;
    this._openai = (params == null ? void 0 : params.openai) || new import_openai2.default({});
    if (params == null ? void 0 : params.model) {
      this.model = params.model;
    }
  }
  get openai() {
    return this._openai;
  }
  async getResponse(forwardedProps) {
    forwardedProps = { ...forwardedProps };
    if (forwardedProps.tools && forwardedProps.tools.length === 0) {
      delete forwardedProps.tools;
    }
    const messages = limitOpenAIMessagesToTokenCount(
      forwardedProps.messages || [],
      forwardedProps.tools || [],
      maxTokensForOpenAIModel(forwardedProps.model || this.model)
    );
    const stream = this.openai.beta.chat.completions.stream({
      model: this.model,
      ...forwardedProps,
      stream: true,
      messages
    }).toReadableStream();
    return { stream };
  }
};

// src/lib/langchain-adapter.ts
var import_messages = require("@langchain/core/messages");
var LangChainAdapter = class {
  constructor(chainFn) {
    this.chainFn = chainFn;
  }
  async getResponse(forwardedProps) {
    var _a, _b, _c;
    forwardedProps = this.transformProps(forwardedProps);
    const result = await this.chainFn(forwardedProps);
    if (typeof result === "string") {
      return {
        stream: new SingleChunkReadableStream(result)
      };
    } else if ("content" in result && typeof result.content === "string") {
      return {
        stream: new SingleChunkReadableStream(result.content, (_a = result.additional_kwargs) == null ? void 0 : _a.tool_calls)
      };
    } else if ("lc_kwargs" in result) {
      return {
        stream: new SingleChunkReadableStream(
          (_b = result.lc_kwargs) == null ? void 0 : _b.content,
          (_c = result.lc_kwargs) == null ? void 0 : _c.tool_calls
        )
      };
    } else if ("getReader" in result) {
      return {
        stream: this.streamResult(result)
      };
    }
    console.error("Invalid return type from LangChain function.");
    throw new Error("Invalid return type from LangChain function.");
  }
  /**
   * Transforms the props that are forwarded to the LangChain function.
   * Currently this just transforms the messages to the format that LangChain expects.
   *
   * @param forwardedProps
   * @returns {any}
   */
  transformProps(forwardedProps) {
    const forwardedPropsCopy = Object.assign({}, forwardedProps);
    if (forwardedProps.messages && Array.isArray(forwardedProps.messages)) {
      const newMessages = [];
      for (const message of forwardedProps.messages) {
        if (message.role === "user") {
          newMessages.push(new import_messages.HumanMessage(message.content));
        } else if (message.role === "assistant") {
          newMessages.push(new import_messages.AIMessage(message.content));
        } else if (message.role === "system") {
          newMessages.push(new import_messages.SystemMessage(message.content));
        }
      }
      forwardedPropsCopy.messages = newMessages;
    }
    return forwardedPropsCopy;
  }
  /**
   * Reads from the LangChainMessageStream and converts the output to a ReadableStream.
   *
   * @param streamedChain
   * @returns ReadableStream
   */
  streamResult(streamedChain) {
    let reader = streamedChain.getReader();
    async function cleanup(controller) {
      if (controller) {
        try {
          controller.close();
        } catch (_) {
        }
      }
      if (reader) {
        try {
          await reader.cancel();
        } catch (_) {
        }
      }
    }
    return new ReadableStream({
      async pull(controller) {
        var _a, _b, _c;
        while (true) {
          try {
            const { done, value } = await reader.read();
            if (done) {
              writeChatCompletionEnd(controller);
              await cleanup(controller);
              return;
            }
            const toolCalls = (_b = (_a = value.lc_kwargs) == null ? void 0 : _a.additional_kwargs) == null ? void 0 : _b.tool_calls;
            const content = (_c = value == null ? void 0 : value.lc_kwargs) == null ? void 0 : _c.content;
            const chunk = {
              choices: [
                {
                  delta: {
                    role: "assistant",
                    content,
                    ...toolCalls ? { tool_calls: toolCalls } : {}
                  }
                }
              ]
            };
            writeChatCompletionChunk(controller, chunk);
          } catch (error) {
            controller.error(error);
            await cleanup(controller);
            return;
          }
        }
      },
      cancel() {
        cleanup();
      }
    });
  }
};
var SingleChunkReadableStream = class extends ReadableStream {
  constructor(content = "", toolCalls) {
    super({
      start(controller) {
        const chunk = {
          choices: [
            {
              delta: {
                role: "assistant",
                content,
                ...toolCalls ? { tool_calls: toolCalls } : {}
              }
            }
          ]
        };
        writeChatCompletionChunk(controller, chunk);
        writeChatCompletionEnd(controller);
        controller.close();
      },
      cancel() {
      }
    });
  }
};

// src/lib/openai-assistant-adapter.ts
var import_openai4 = __toESM(require("openai"));
var RUN_STATUS_POLL_INTERVAL = 100;
var OpenAIAssistantAdapter = class {
  constructor(params) {
    this.openai = params.openai || new import_openai4.default({});
    this.codeInterpreterEnabled = params.codeInterpreterEnabled === false || true;
    this.retrievalEnabled = params.retrievalEnabled === false || true;
    this.assistantId = params.assistantId;
  }
  async waitForRun(run) {
    while (true) {
      const status = await this.openai.beta.threads.runs.retrieve(run.thread_id, run.id);
      if (status.status === "completed" || status.status === "requires_action") {
        return status;
      } else if (status.status !== "in_progress" && status.status !== "queued") {
        console.error(`Thread run failed with status: ${status.status}`);
        throw new Error(`Thread run failed with status: ${status.status}`);
      }
      await new Promise((resolve) => setTimeout(resolve, RUN_STATUS_POLL_INTERVAL));
    }
  }
  async submitToolOutputs(threadId, runId, forwardMessages) {
    let run = await this.openai.beta.threads.runs.retrieve(threadId, runId);
    if (!run.required_action) {
      throw new Error("No tool outputs required");
    }
    const functionResults = [];
    let i = forwardMessages.length - 1;
    for (; i >= 0; i--) {
      if (forwardMessages[i].role === "function") {
        functionResults.unshift(forwardMessages[i]);
      } else {
        break;
      }
    }
    const toolCallsIds = run.required_action.submit_tool_outputs.tool_calls.map(
      (toolCall) => toolCall.id
    );
    if (toolCallsIds.length != functionResults.length) {
      throw new Error("Number of function results does not match the number of tool calls");
    }
    const toolOutputs = [];
    for (let i2 = 0; i2 < functionResults.length; i2++) {
      const toolCallId = toolCallsIds[i2];
      const functionResult = functionResults[i2];
      toolOutputs.push({
        tool_call_id: toolCallId,
        output: functionResult.content || ""
      });
    }
    run = await this.openai.beta.threads.runs.submitToolOutputs(threadId, runId, {
      tool_outputs: toolOutputs
    });
    return await this.waitForRun(run);
  }
  async submitUserMessage(threadId, forwardedProps) {
    const forwardMessages = forwardedProps.messages || [];
    const message = forwardMessages[forwardMessages.length - 1];
    await this.openai.beta.threads.messages.create(threadId, {
      role: message.role,
      content: message.content
    });
    const tools = [
      ...forwardedProps.tools || [],
      ...this.codeInterpreterEnabled ? [{ type: "code_interpreter" }] : [],
      ...this.retrievalEnabled ? [{ type: "retrieval" }] : []
    ];
    const instructions = forwardMessages.filter((message2) => message2.role === "system").map((message2) => message2.content).join("\n\n");
    let run = await this.openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId,
      instructions,
      tools
    });
    return await this.waitForRun(run);
  }
  async getResponse(forwardedProps) {
    forwardedProps = { ...forwardedProps };
    const forwardMessages = forwardedProps.messages || [];
    if (forwardedProps.tools && forwardedProps.tools.length === 0) {
      delete forwardedProps.tools;
    }
    const threadId = forwardedProps.threadId || (await this.openai.beta.threads.create()).id;
    let run = null;
    if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "function") {
      run = await this.submitToolOutputs(threadId, forwardedProps.runId, forwardMessages);
    } else if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "user") {
      run = await this.submitUserMessage(threadId, forwardedProps);
    } else {
      console.error("No actionable message found in the messages");
      throw new Error("No actionable message found in the messages");
    }
    if (run.status === "requires_action") {
      return {
        stream: new AssistantSingleChunkReadableStream(
          "",
          run.required_action.submit_tool_outputs.tool_calls
        ),
        headers: { threadId, runId: run.id }
      };
    } else {
      const newMessages = await this.openai.beta.threads.messages.list(threadId, {
        limit: 1,
        order: "desc"
      });
      const content = newMessages.data[0].content[0];
      const contentString = content.type === "text" ? content.text.value : "";
      return {
        stream: new AssistantSingleChunkReadableStream(contentString),
        headers: { threadId }
      };
    }
  }
};
var AssistantSingleChunkReadableStream = class extends ReadableStream {
  constructor(content, toolCalls) {
    super({
      start(controller) {
        let tool_calls = void 0;
        if (toolCalls) {
          tool_calls = toolCalls.map((toolCall, index) => {
            return {
              index,
              id: toolCall.id,
              function: {
                name: toolCall.function.name,
                arguments: toolCall.function.arguments
              }
            };
          });
        }
        const chunk = {
          choices: [
            {
              delta: {
                content,
                role: "assistant",
                tool_calls
              }
            }
          ]
        };
        writeChatCompletionChunk(controller, chunk);
        writeChatCompletionEnd(controller);
        controller.close();
      },
      cancel() {
      }
    });
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  CopilotBackend,
  LangChainAdapter,
  OpenAIAdapter,
  OpenAIAssistantAdapter
});
//# sourceMappingURL=index.js.map