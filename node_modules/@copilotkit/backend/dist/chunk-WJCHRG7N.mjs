import {
  writeChatCompletionChunk,
  writeChatCompletionEnd
} from "./chunk-P7GKPATA.mjs";

// src/lib/openai-assistant-adapter.ts
import OpenAI from "openai";
var RUN_STATUS_POLL_INTERVAL = 100;
var OpenAIAssistantAdapter = class {
  constructor(params) {
    this.openai = params.openai || new OpenAI({});
    this.codeInterpreterEnabled = params.codeInterpreterEnabled === false || true;
    this.retrievalEnabled = params.retrievalEnabled === false || true;
    this.assistantId = params.assistantId;
  }
  async waitForRun(run) {
    while (true) {
      const status = await this.openai.beta.threads.runs.retrieve(run.thread_id, run.id);
      if (status.status === "completed" || status.status === "requires_action") {
        return status;
      } else if (status.status !== "in_progress" && status.status !== "queued") {
        console.error(`Thread run failed with status: ${status.status}`);
        throw new Error(`Thread run failed with status: ${status.status}`);
      }
      await new Promise((resolve) => setTimeout(resolve, RUN_STATUS_POLL_INTERVAL));
    }
  }
  async submitToolOutputs(threadId, runId, forwardMessages) {
    let run = await this.openai.beta.threads.runs.retrieve(threadId, runId);
    if (!run.required_action) {
      throw new Error("No tool outputs required");
    }
    const functionResults = [];
    let i = forwardMessages.length - 1;
    for (; i >= 0; i--) {
      if (forwardMessages[i].role === "function") {
        functionResults.unshift(forwardMessages[i]);
      } else {
        break;
      }
    }
    const toolCallsIds = run.required_action.submit_tool_outputs.tool_calls.map(
      (toolCall) => toolCall.id
    );
    if (toolCallsIds.length != functionResults.length) {
      throw new Error("Number of function results does not match the number of tool calls");
    }
    const toolOutputs = [];
    for (let i2 = 0; i2 < functionResults.length; i2++) {
      const toolCallId = toolCallsIds[i2];
      const functionResult = functionResults[i2];
      toolOutputs.push({
        tool_call_id: toolCallId,
        output: functionResult.content || ""
      });
    }
    run = await this.openai.beta.threads.runs.submitToolOutputs(threadId, runId, {
      tool_outputs: toolOutputs
    });
    return await this.waitForRun(run);
  }
  async submitUserMessage(threadId, forwardedProps) {
    const forwardMessages = forwardedProps.messages || [];
    const message = forwardMessages[forwardMessages.length - 1];
    await this.openai.beta.threads.messages.create(threadId, {
      role: message.role,
      content: message.content
    });
    const tools = [
      ...forwardedProps.tools || [],
      ...this.codeInterpreterEnabled ? [{ type: "code_interpreter" }] : [],
      ...this.retrievalEnabled ? [{ type: "retrieval" }] : []
    ];
    const instructions = forwardMessages.filter((message2) => message2.role === "system").map((message2) => message2.content).join("\n\n");
    let run = await this.openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId,
      instructions,
      tools
    });
    return await this.waitForRun(run);
  }
  async getResponse(forwardedProps) {
    forwardedProps = { ...forwardedProps };
    const forwardMessages = forwardedProps.messages || [];
    if (forwardedProps.tools && forwardedProps.tools.length === 0) {
      delete forwardedProps.tools;
    }
    const threadId = forwardedProps.threadId || (await this.openai.beta.threads.create()).id;
    let run = null;
    if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "function") {
      run = await this.submitToolOutputs(threadId, forwardedProps.runId, forwardMessages);
    } else if (forwardMessages.length > 0 && forwardMessages[forwardMessages.length - 1].role === "user") {
      run = await this.submitUserMessage(threadId, forwardedProps);
    } else {
      console.error("No actionable message found in the messages");
      throw new Error("No actionable message found in the messages");
    }
    if (run.status === "requires_action") {
      return {
        stream: new AssistantSingleChunkReadableStream(
          "",
          run.required_action.submit_tool_outputs.tool_calls
        ),
        headers: { threadId, runId: run.id }
      };
    } else {
      const newMessages = await this.openai.beta.threads.messages.list(threadId, {
        limit: 1,
        order: "desc"
      });
      const content = newMessages.data[0].content[0];
      const contentString = content.type === "text" ? content.text.value : "";
      return {
        stream: new AssistantSingleChunkReadableStream(contentString),
        headers: { threadId }
      };
    }
  }
};
var AssistantSingleChunkReadableStream = class extends ReadableStream {
  constructor(content, toolCalls) {
    super({
      start(controller) {
        let tool_calls = void 0;
        if (toolCalls) {
          tool_calls = toolCalls.map((toolCall, index) => {
            return {
              index,
              id: toolCall.id,
              function: {
                name: toolCall.function.name,
                arguments: toolCall.function.arguments
              }
            };
          });
        }
        const chunk = {
          choices: [
            {
              delta: {
                content,
                role: "assistant",
                tool_calls
              }
            }
          ]
        };
        writeChatCompletionChunk(controller, chunk);
        writeChatCompletionEnd(controller);
        controller.close();
      },
      cancel() {
      }
    });
  }
};

export {
  OpenAIAssistantAdapter
};
//# sourceMappingURL=chunk-WJCHRG7N.mjs.map